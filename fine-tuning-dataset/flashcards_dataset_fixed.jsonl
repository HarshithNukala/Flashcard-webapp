{"id": "qa_001", "task_type": "qa", "context": "A stack is a linear data structure that follows the Last In, First Out principle. The element that is inserted last is removed first. Common stack operations are push to add an element, pop to remove the top element, and peek to view the top element without removing it.", "question": "What is a stack in data structures and how does it work?", "target": {"answer": "A stack is a linear data structure that follows the Last In, First Out principle, where the last inserted element is removed first using operations like push, pop, and peek.", "sources": ["A stack is a linear data structure that follows the Last In, First Out principle.", "Common stack operations are push to add an element, pop to remove the top element, and peek to view the top element without removing it."]}}
{"id": "qa_002", "task_type": "qa", "context": "A queue is a linear data structure that follows the First In, First Out principle. The element that is inserted first is removed first. The main operations are enqueue to insert an element at the rear and dequeue to remove an element from the front.", "question": "What is a queue in data structures and what principle does it follow?", "target": {"answer": "A queue is a linear data structure that follows the First In, First Out principle, where the first inserted element is the first one to be removed using enqueue and dequeue operations.", "sources": ["A queue is a linear data structure that follows the First In, First Out principle.", "The main operations are enqueue to insert an element at the rear and dequeue to remove an element from the front."]}}
{"id": "qa_003", "task_type": "qa", "context": "In computer science, a binary tree is a hierarchical data structure in which each node has at most two children, commonly referred to as the left child and the right child. Binary trees are used to implement binary search trees and heaps.", "question": "What is a binary tree in computer science?", "target": {"answer": "A binary tree is a hierarchical data structure where each node has at most two children, typically called the left child and the right child.", "sources": ["In computer science, a binary tree is a hierarchical data structure in which each node has at most two children, commonly referred to as the left child and the right child."]}}
{"id": "qa_004", "task_type": "qa", "context": "Big O notation is used to describe the upper bound of the time or space complexity of an algorithm in terms of input size. It characterizes how the runtime or memory usage grows as the input size increases, ignoring constant factors and lower-order terms.", "question": "What does Big O notation describe in the context of algorithms?", "target": {"answer": "Big O notation describes the upper bound of an algorithm's time or space complexity in terms of input size, showing how resource usage grows as the input size increases.", "sources": ["Big O notation is used to describe the upper bound of the time or space complexity of an algorithm in terms of input size.", "It characterizes how the runtime or memory usage grows as the input size increases, ignoring constant factors and lower-order terms."]}}
{"id": "qa_005", "task_type": "qa", "context": "Overfitting occurs in machine learning when a model learns the training data too well, including its noise and outliers. As a result, the model performs very well on the training set but poorly on unseen test data because it fails to generalize.", "question": "What is overfitting in machine learning?", "target": {"answer": "Overfitting is when a machine learning model learns the training data too closely, including noise and outliers, causing it to perform well on training data but poorly on unseen test data.", "sources": ["Overfitting occurs in machine learning when a model learns the training data too well, including its noise and outliers.", "As a result, the model performs very well on the training set but poorly on unseen test data because it fails to generalize."]}}
{"id": "qa_006", "task_type": "qa", "context": "Underfitting happens when a machine learning model is too simple to capture the underlying patterns in the data. The model performs poorly on both the training data and unseen data because it has not learned enough from the training set.", "question": "What is underfitting in machine learning?", "target": {"answer": "Underfitting is when a machine learning model is too simple to capture the patterns in the data and therefore performs poorly on both the training and test sets.", "sources": ["Underfitting happens when a machine learning model is too simple to capture the underlying patterns in the data.", "The model performs poorly on both the training data and unseen data because it has not learned enough from the training set."]}}
{"id": "qa_007", "task_type": "qa", "context": "In supervised learning, a model is trained using labeled data, where each training example consists of an input and a known output. The goal is to learn a mapping from inputs to outputs so that the model can predict labels for new, unseen data.", "question": "What is the goal of supervised learning?", "target": {"answer": "The goal of supervised learning is to learn a mapping from inputs to outputs using labeled data so that the model can predict labels for new, unseen data.", "sources": ["In supervised learning, a model is trained using labeled data, where each training example consists of an input and a known output.", "The goal is to learn a mapping from inputs to outputs so that the model can predict labels for new, unseen data."]}}
{"id": "qa_008", "task_type": "qa", "context": "Unsupervised learning is a type of machine learning where the model is trained on unlabeled data. The objective is to discover hidden patterns or structures in the data, such as clusters or associations, without being given explicit output labels.", "question": "What is unsupervised learning and what is its main objective?", "target": {"answer": "Unsupervised learning is a type of machine learning that uses unlabeled data, and its main objective is to discover hidden patterns or structures like clusters or associations in the data.", "sources": ["Unsupervised learning is a type of machine learning where the model is trained on unlabeled data.", "The objective is to discover hidden patterns or structures in the data, such as clusters or associations, without being given explicit output labels."]}}
{"id": "qa_009", "task_type": "qa", "context": "Gradient descent is an optimization algorithm used to minimize a loss function in machine learning by iteratively updating parameters in the opposite direction of the gradient. The learning rate controls the size of each update step.", "question": "What is gradient descent used for in machine learning?", "target": {"answer": "Gradient descent is an optimization algorithm used to minimize a loss function by iteratively updating model parameters in the opposite direction of the gradient.", "sources": ["Gradient descent is an optimization algorithm used to minimize a loss function in machine learning by iteratively updating parameters in the opposite direction of the gradient."]}}
{"id": "qa_010", "task_type": "qa", "context": "A learning rate in gradient-based optimization determines how large the parameter updates are in each iteration. If the learning rate is too high, the training process can become unstable. If it is too low, training can become very slow and may get stuck in local minima.", "question": "What is the role of the learning rate in gradient descent?", "target": {"answer": "The learning rate controls the size of parameter updates in gradient descent, where too high a value can make training unstable and too low a value can make training slow or get stuck.", "sources": ["A learning rate in gradient-based optimization determines how large the parameter updates are in each iteration.", "If the learning rate is too high, the training process can become unstable. If it is too low, training can become very slow and may get stuck in local minima."]}}
{"id": "qa_011", "task_type": "qa", "context": "Precision in classification is the ratio of true positives to the sum of true positives and false positives. It measures how many of the predicted positive instances are actually positive.", "question": "What does precision measure in a classification model?", "target": {"answer": "Precision measures the ratio of true positives to the sum of true positives and false positives, indicating how many predicted positives are actually positive.", "sources": ["Precision in classification is the ratio of true positives to the sum of true positives and false positives.", "It measures how many of the predicted positive instances are actually positive."]}}
{"id": "qa_012", "task_type": "qa", "context": "Recall in classification is the ratio of true positives to the sum of true positives and false negatives. It measures how many of the actual positive instances the model correctly identifies.", "question": "What does recall measure in a classification model?", "target": {"answer": "Recall measures the ratio of true positives to the sum of true positives and false negatives, indicating how many actual positives the model correctly identifies.", "sources": ["Recall in classification is the ratio of true positives to the sum of true positives and false negatives.", "It measures how many of the actual positive instances the model correctly identifies."]}}
{"id": "qa_013", "task_type": "qa", "context": "F1 score is the harmonic mean of precision and recall in a classification model. It provides a single metric that balances both precision and recall, especially useful when the class distribution is imbalanced.", "question": "What is the F1 score and why is it useful?", "target": {"answer": "The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both and is especially useful for evaluating models on imbalanced datasets.", "sources": ["F1 score is the harmonic mean of precision and recall in a classification model.", "It provides a single metric that balances both precision and recall, especially useful when the class distribution is imbalanced."]}}
{"id": "qa_014", "task_type": "qa", "context": "In databases, normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves decomposing tables into smaller, related tables and defining relationships between them.", "question": "What is normalization in databases and why is it used?", "target": {"answer": "Normalization is the process of organizing data in a database to reduce redundancy and improve data integrity by decomposing tables into smaller related tables and defining relationships between them.", "sources": ["In databases, normalization is the process of organizing data to reduce redundancy and improve data integrity.", "It involves decomposing tables into smaller, related tables and defining relationships between them."]}}
{"id": "qa_015", "task_type": "qa", "context": "A primary key in a relational database is a field or combination of fields that uniquely identifies each record in a table. A primary key must contain unique values and cannot contain null values.", "question": "What is a primary key in a relational database?", "target": {"answer": "A primary key is a field or combination of fields in a relational database table that uniquely identifies each record and cannot contain null values.", "sources": ["A primary key in a relational database is a field or combination of fields that uniquely identifies each record in a table.", "A primary key must contain unique values and cannot contain null values."]}}
{"id": "qa_016", "task_type": "qa", "context": "A foreign key in a relational database is a field or set of fields in one table that refers to the primary key in another table. It is used to enforce referential integrity between related tables.", "question": "What is a foreign key and why is it important?", "target": {"answer": "A foreign key is a field or set of fields in one table that refers to the primary key in another table, and it is important for enforcing referential integrity between related tables.", "sources": ["A foreign key in a relational database is a field or set of fields in one table that refers to the primary key in another table.", "It is used to enforce referential integrity between related tables."]}}
{"id": "qa_017", "task_type": "qa", "context": "An operating system is system software that manages computer hardware and software resources and provides common services for computer programs. It handles tasks such as process management, memory management, file systems, and input output control.", "question": "What is an operating system and what does it do?", "target": {"answer": "An operating system is system software that manages computer hardware and software resources and provides common services like process management, memory management, file systems, and input output control.", "sources": ["An operating system is system software that manages computer hardware and software resources and provides common services for computer programs.", "It handles tasks such as process management, memory management, file systems, and input output control."]}}
{"id": "qa_018", "task_type": "qa", "context": "In computer networks, latency refers to the time it takes for data to travel from the source to the destination. High latency can cause noticeable delays in communication, especially in real-time applications like video calls and online gaming.", "question": "What is latency in computer networks?", "target": {"answer": "Latency in computer networks is the time it takes for data to travel from the source to the destination, where high latency causes noticeable delays in communication.", "sources": ["In computer networks, latency refers to the time it takes for data to travel from the source to the destination.", "High latency can cause noticeable delays in communication, especially in real-time applications like video calls and online gaming."]}}
{"id": "qa_019", "task_type": "qa", "context": "Bandwidth in networking is the maximum rate at which data can be transmitted over a communication channel, usually measured in bits per second. Higher bandwidth allows more data to be sent in a given amount of time.", "question": "What is bandwidth in the context of computer networks?", "target": {"answer": "Bandwidth is the maximum data transmission rate of a communication channel, typically measured in bits per second, and higher bandwidth allows more data to be sent in a given time.", "sources": ["Bandwidth in networking is the maximum rate at which data can be transmitted over a communication channel, usually measured in bits per second.", "Higher bandwidth allows more data to be sent in a given amount of time."]}}
{"id": "qa_020", "task_type": "qa", "context": "The Transmission Control Protocol is a connection-oriented protocol that provides reliable, ordered, and error-checked delivery of data between applications. It establishes a connection before data transfer and ensures that data arrives without errors and in the correct order.", "question": "What is the main role of the Transmission Control Protocol?", "target": {"answer": "The Transmission Control Protocol provides reliable, ordered, and error-checked delivery of data between applications by establishing a connection before data transfer.", "sources": ["The Transmission Control Protocol is a connection-oriented protocol that provides reliable, ordered, and error-checked delivery of data between applications.", "It establishes a connection before data transfer and ensures that data arrives without errors and in the correct order."]}}
{"id": "qa_021", "task_type": "qa", "context": "HTTP, or Hypertext Transfer Protocol, is an application-level protocol used for transferring hypertext documents such as HTML. It is the foundation of data communication on the World Wide Web and follows a client server model.", "question": "What is HTTP and what is it used for?", "target": {"answer": "HTTP is the Hypertext Transfer Protocol, an application-level protocol used for transferring hypertext documents like HTML and forming the basis of data communication on the World Wide Web.", "sources": ["HTTP, or Hypertext Transfer Protocol, is an application-level protocol used for transferring hypertext documents such as HTML.", "It is the foundation of data communication on the World Wide Web and follows a client server model."]}}
{"id": "qa_022", "task_type": "qa", "context": "A compiler is a program that translates source code written in a high-level programming language into machine code or an intermediate form that can be executed by a computer. The compilation process typically includes lexical analysis, parsing, semantic analysis, optimization, and code generation.", "question": "What is a compiler and what does it do?", "target": {"answer": "A compiler is a program that translates high-level source code into machine code or an intermediate form that can be executed by a computer, performing steps like lexical analysis, parsing, semantic analysis, optimization, and code generation.", "sources": ["A compiler is a program that translates source code written in a high-level programming language into machine code or an intermediate form that can be executed by a computer.", "The compilation process typically includes lexical analysis, parsing, semantic analysis, optimization, and code generation."]}}
{"id": "qa_023", "task_type": "qa", "context": "An interpreter is a program that directly executes instructions written in a high-level programming language without first compiling them into machine code. It reads the source code line by line, translates it, and immediately executes it.", "question": "What is an interpreter in programming languages?", "target": {"answer": "An interpreter is a program that executes high-level source code directly by reading it line by line, translating it, and immediately running the instructions without producing separate machine code.", "sources": ["An interpreter is a program that directly executes instructions written in a high-level programming language without first compiling them into machine code.", "It reads the source code line by line, translates it, and immediately executes it."]}}
{"id": "qa_024", "task_type": "qa", "context": "Time complexity of an algorithm describes how the running time grows as the size of the input increases. It is often expressed using Big O notation, which focuses on the dominant term and ignores constant factors.", "question": "What does time complexity describe for an algorithm?", "target": {"answer": "Time complexity describes how an algorithm's running time grows as the input size increases, usually expressed in Big O notation focusing on the dominant term.", "sources": ["Time complexity of an algorithm describes how the running time grows as the size of the input increases.", "It is often expressed using Big O notation, which focuses on the dominant term and ignores constant factors."]}}
{"id": "qa_025", "task_type": "qa", "context": "Space complexity of an algorithm measures the amount of memory it uses as a function of input size. It includes memory for input, output, and auxiliary variables used during computation.", "question": "What is space complexity in algorithms?", "target": {"answer": "Space complexity is a measure of how much memory an algorithm uses as a function of input size, including memory for input, output, and auxiliary variables.", "sources": ["Space complexity of an algorithm measures the amount of memory it uses as a function of input size.", "It includes memory for input, output, and auxiliary variables used during computation."]}}
{"id": "qa_026", "task_type": "qa", "context": "A hash table is a data structure that maps keys to values using a hash function. The hash function computes an index into an array of buckets, from which the desired value can be found. Hash tables provide average case constant time complexity for search, insert, and delete operations.", "question": "What is a hash table and what is its typical time complexity for basic operations?", "target": {"answer": "A hash table is a data structure that maps keys to values using a hash function to compute an index into an array of buckets, and it typically offers average case constant time for search, insert, and delete operations.", "sources": ["A hash table is a data structure that maps keys to values using a hash function.", "The hash function computes an index into an array of buckets, from which the desired value can be found.", "Hash tables provide average case constant time complexity for search, insert, and delete operations."]}}
{"id": "qa_027", "task_type": "qa", "context": "In object-oriented programming, encapsulation is the concept of bundling data and methods that operate on that data within a single unit, typically a class. It restricts direct access to some of an object's components, which helps protect the integrity of the data.", "question": "What is encapsulation in object-oriented programming?", "target": {"answer": "Encapsulation is the practice of bundling data and the methods that operate on it within a single unit, such as a class, while restricting direct access to some of the object's components to protect data integrity.", "sources": ["In object-oriented programming, encapsulation is the concept of bundling data and methods that operate on that data within a single unit, typically a class.", "It restricts direct access to some of an object's components, which helps protect the integrity of the data."]}}
{"id": "qa_028", "task_type": "qa", "context": "Inheritance in object-oriented programming allows a class to acquire the properties and behaviors of another class. The class that is inherited from is called the base or parent class, and the class that inherits is called the derived or child class.", "question": "What is inheritance in object-oriented programming?", "target": {"answer": "Inheritance is a feature where a class, called a derived or child class, acquires the properties and behaviors of another class, called the base or parent class.", "sources": ["Inheritance in object-oriented programming allows a class to acquire the properties and behaviors of another class.", "The class that is inherited from is called the base or parent class, and the class that inherits is called the derived or child class."]}}
{"id": "qa_029", "task_type": "qa", "context": "Polymorphism in object-oriented programming is the ability of different classes to be treated through the same interface, where the same operation can behave differently on different classes. This is often implemented through method overriding and interfaces.", "question": "What is polymorphism in object-oriented programming?", "target": {"answer": "Polymorphism is the ability to treat different classes through the same interface so that the same operation can behave differently on different classes, often implemented via method overriding and interfaces.", "sources": ["Polymorphism in object-oriented programming is the ability of different classes to be treated through the same interface, where the same operation can behave differently on different classes.", "This is often implemented through method overriding and interfaces."]}}
{"id": "qa_030", "task_type": "qa", "context": "Regularization in machine learning refers to techniques that add a penalty to the loss function to discourage complex models. By penalizing large parameter values, regularization helps prevent overfitting and improves generalization to new data.", "question": "What is regularization in machine learning and why is it used?", "target": {"answer": "Regularization refers to techniques that add a penalty to the loss function to discourage overly complex models, helping to prevent overfitting and improve generalization.", "sources": ["Regularization in machine learning refers to techniques that add a penalty to the loss function to discourage complex models.", "By penalizing large parameter values, regularization helps prevent overfitting and improves generalization to new data."]}}
{"id": "qa_031", "task_type": "qa", "context": "Dropout is a regularization technique used in neural networks where, during training, randomly selected neurons are temporarily ignored. This prevents neurons from co-adapting too much and helps the network generalize better to unseen data.", "question": "What is dropout in neural networks?", "target": {"answer": "Dropout is a regularization technique in neural networks where randomly selected neurons are ignored during training to prevent co-adaptation and improve generalization.", "sources": ["Dropout is a regularization technique used in neural networks where, during training, randomly selected neurons are temporarily ignored.", "This prevents neurons from co-adapting too much and helps the network generalize better to unseen data."]}}
{"id": "qa_032", "task_type": "qa", "context": "A learning curve in machine learning plots the model's performance on the training and validation sets as a function of the training iterations or training data size. It is used to diagnose issues like overfitting and underfitting.", "question": "What is a learning curve in machine learning used for?", "target": {"answer": "A learning curve shows model performance on training and validation sets over training iterations or data size and is used to diagnose overfitting and underfitting.", "sources": ["A learning curve in machine learning plots the model's performance on the training and validation sets as a function of the training iterations or training data size.", "It is used to diagnose issues like overfitting and underfitting."]}}
{"id": "qa_033", "task_type": "qa", "context": "Cross validation is a technique for evaluating machine learning models by splitting the data into multiple folds. The model is trained on some folds and validated on the remaining fold, and this process is repeated so that each fold serves as validation once. The results are averaged to estimate model performance.", "question": "What is cross validation and why is it used?", "target": {"answer": "Cross validation is a method of evaluating a model by splitting the data into multiple folds, training on some folds and validating on others, and averaging the results to get a more reliable estimate of performance.", "sources": ["Cross validation is a technique for evaluating machine learning models by splitting the data into multiple folds.", "The model is trained on some folds and validated on the remaining fold, and this process is repeated so that each fold serves as validation once.", "The results are averaged to estimate model performance."]}}
{"id": "qa_034", "task_type": "qa", "context": "The bias variance trade off describes the balance between underfitting and overfitting in machine learning models. High bias models are too simple and underfit, while high variance models are too complex and overfit. The goal is to find a model with an appropriate balance.", "question": "What is the bias variance trade off in machine learning?", "target": {"answer": "The bias variance trade off is the balance between underfitting and overfitting, where high bias models are too simple and underfit and high variance models are too complex and overfit, and the goal is to find a balanced model.", "sources": ["The bias variance trade off describes the balance between underfitting and overfitting in machine learning models.", "High bias models are too simple and underfit, while high variance models are too complex and overfit.", "The goal is to find a model with an appropriate balance."]}}
{"id": "qa_035", "task_type": "qa", "context": "In natural language processing, tokenization is the process of breaking text into smaller units called tokens. Tokens can be words, subwords, or characters, and they are the basic units that models operate on.", "question": "What is tokenization in natural language processing?", "target": {"answer": "Tokenization is the process of splitting text into smaller units called tokens, such as words, subwords, or characters, which serve as the basic units for NLP models.", "sources": ["In natural language processing, tokenization is the process of breaking text into smaller units called tokens.", "Tokens can be words, subwords, or characters, and they are the basic units that models operate on."]}}
{"id": "qa_036", "task_type": "qa", "context": "Stemming is a text normalization technique that reduces words to their root form by stripping suffixes. For example, running, runs, and runner might all be reduced to run. Stemming can be aggressive and does not always produce real words.", "question": "What is stemming in text processing?", "target": {"answer": "Stemming is a text normalization method that reduces words to a root form by stripping suffixes, such as turning running, runs, and runner into run, though the result may not always be a real word.", "sources": ["Stemming is a text normalization technique that reduces words to their root form by stripping suffixes.", "For example, running, runs, and runner might all be reduced to run.", "Stemming can be aggressive and does not always produce real words."]}}
{"id": "qa_037", "task_type": "qa", "context": "Lemmatization is a text normalization technique that reduces words to their base or dictionary form, called a lemma, using vocabulary and morphological analysis. Unlike stemming, lemmatization usually returns valid words.", "question": "How does lemmatization differ from stemming?", "target": {"answer": "Lemmatization reduces words to their base or dictionary form using vocabulary and morphological analysis and usually returns valid words, whereas stemming simply strips suffixes and may produce nonwords.", "sources": ["Lemmatization is a text normalization technique that reduces words to their base or dictionary form, called a lemma, using vocabulary and morphological analysis.", "Unlike stemming, lemmatization usually returns valid words."]}}
{"id": "qa_038", "task_type": "qa", "context": "A confusion matrix is a table that summarizes the performance of a classification model by comparing predicted labels to true labels. It shows true positives, false positives, true negatives, and false negatives.", "question": "What is a confusion matrix in classification?", "target": {"answer": "A confusion matrix is a table that summarizes a classification model's performance by showing counts of true positives, false positives, true negatives, and false negatives.", "sources": ["A confusion matrix is a table that summarizes the performance of a classification model by comparing predicted labels to true labels.", "It shows true positives, false positives, true negatives, and false negatives."]}}
{"id": "qa_039", "task_type": "qa", "context": "Principal component analysis is a dimensionality reduction technique that transforms the original features into a smaller set of uncorrelated components. These components capture the maximum variance in the data.", "question": "What is principal component analysis used for?", "target": {"answer": "Principal component analysis is used for dimensionality reduction by transforming original features into a smaller set of uncorrelated components that capture most of the variance in the data.", "sources": ["Principal component analysis is a dimensionality reduction technique that transforms the original features into a smaller set of uncorrelated components.", "These components capture the maximum variance in the data."]}}
{"id": "qa_040", "task_type": "qa", "context": "A learning rate scheduler adjusts the learning rate during training according to a predefined strategy. For example, it may reduce the learning rate when the validation loss stops improving, which can help the model converge more effectively.", "question": "What is the purpose of a learning rate scheduler?", "target": {"answer": "A learning rate scheduler automatically adjusts the learning rate during training, often reducing it when validation performance stops improving to help the model converge better.", "sources": ["A learning rate scheduler adjusts the learning rate during training according to a predefined strategy.", "For example, it may reduce the learning rate when the validation loss stops improving, which can help the model converge more effectively."]}}
{"id": "qa_041", "task_type": "qa", "context": "Batch normalization is a technique used to normalize the inputs of each layer in a neural network within a mini batch. It helps stabilize and accelerate training by reducing internal covariate shift.", "question": "What is batch normalization in neural networks?", "target": {"answer": "Batch normalization normalizes the inputs of each layer within a mini batch to reduce internal covariate shift, helping to stabilize and speed up neural network training.", "sources": ["Batch normalization is a technique used to normalize the inputs of each layer in a neural network within a mini batch.", "It helps stabilize and accelerate training by reducing internal covariate shift."]}}
{"id": "qa_042", "task_type": "qa", "context": "Over-parameterization in deep learning refers to models that have more parameters than strictly needed to fit the training data. Surprisingly, such models can still generalize well when combined with appropriate regularization and optimization techniques.", "question": "What is over-parameterization in deep learning?", "target": {"answer": "Over-parameterization occurs when a deep learning model has more parameters than needed to fit the training data, yet it can still generalize well with proper regularization and optimization.", "sources": ["Over-parameterization in deep learning refers to models that have more parameters than strictly needed to fit the training data.", "Surprisingly, such models can still generalize well when combined with appropriate regularization and optimization techniques."]}}
{"id": "qa_043", "task_type": "qa", "context": "The k in k-nearest neighbors is a hyperparameter that specifies how many neighbors to look at when making a prediction. A small k can lead to noisy decisions, while a large k can oversmooth class boundaries.", "question": "What does the hyperparameter k control in k-nearest neighbors?", "target": {"answer": "In k-nearest neighbors, k controls how many neighbors are considered when making a prediction, where small k can cause noisy decisions and large k can oversmooth class boundaries.", "sources": ["The k in k-nearest neighbors is a hyperparameter that specifies how many neighbors to look at when making a prediction.", "A small k can lead to noisy decisions, while a large k can oversmooth class boundaries."]}}
{"id": "qa_044", "task_type": "qa", "context": "A decision tree is a model that makes predictions by learning decision rules inferred from features. It splits the data into subsets based on feature values, forming a tree where internal nodes represent tests and leaves represent predictions.", "question": "What is a decision tree model?", "target": {"answer": "A decision tree is a model that predicts outcomes by learning decision rules that split data based on feature values, forming a tree with internal test nodes and leaf prediction nodes.", "sources": ["A decision tree is a model that makes predictions by learning decision rules inferred from features.", "It splits the data into subsets based on feature values, forming a tree where internal nodes represent tests and leaves represent predictions."]}}
{"id": "qa_045", "task_type": "qa", "context": "Bagging, or bootstrap aggregating, is an ensemble learning method that trains multiple models on different bootstrap samples of the training data and then aggregates their predictions. It reduces variance and helps prevent overfitting.", "question": "What is bagging in ensemble learning?", "target": {"answer": "Bagging is an ensemble method that trains multiple models on different bootstrap samples of the training data and aggregates their predictions to reduce variance and prevent overfitting.", "sources": ["Bagging, or bootstrap aggregating, is an ensemble learning method that trains multiple models on different bootstrap samples of the training data and then aggregates their predictions.", "It reduces variance and helps prevent overfitting."]}}
{"id": "qa_046", "task_type": "qa", "context": "Boosting is an ensemble technique that combines many weak learners into a strong learner by training them sequentially. Each new learner focuses on the errors of the previous learners to improve overall performance.", "question": "What is boosting in machine learning?", "target": {"answer": "Boosting is an ensemble technique in which multiple weak learners are trained sequentially, with each new learner focusing on correcting the errors of the previous ones to build a strong overall model.", "sources": ["Boosting is an ensemble technique that combines many weak learners into a strong learner by training them sequentially.", "Each new learner focuses on the errors of the previous learners to improve overall performance."]}}
{"id": "qa_047", "task_type": "qa", "context": "In k-means clustering, the objective is to partition the data into k clusters so that the within-cluster sum of squared distances between points and their cluster centroids is minimized. The algorithm alternates between assigning points to the nearest centroid and recomputing centroids.", "question": "What is the objective of k-means clustering?", "target": {"answer": "The objective of k-means clustering is to partition data into k clusters by minimizing the within-cluster sum of squared distances between points and their cluster centroids.", "sources": ["In k-means clustering, the objective is to partition the data into k clusters so that the within-cluster sum of squared distances between points and their cluster centroids is minimized."]}}
{"id": "qa_048", "task_type": "qa", "context": "The silhouette score measures how similar an object is to its own cluster compared to other clusters. It ranges from minus one to one, with higher values indicating that the object is well matched to its own cluster and poorly matched to neighboring clusters.", "question": "What does the silhouette score indicate in clustering?", "target": {"answer": "The silhouette score indicates how well an object fits within its assigned cluster compared to other clusters, with higher values showing better clustering quality.", "sources": ["The silhouette score measures how similar an object is to its own cluster compared to other clusters.", "It ranges from minus one to one, with higher values indicating that the object is well matched to its own cluster and poorly matched to neighboring clusters."]}}
{"id": "qa_049", "task_type": "qa", "context": "Hyperparameters are configuration values that are set before training a machine learning model and are not learned from data. Examples include learning rate, number of layers, and regularization strength.", "question": "What are hyperparameters in a machine learning model?", "target": {"answer": "Hyperparameters are configuration values set before training, such as learning rate, number of layers, and regularization strength, that are not learned from the data.", "sources": ["Hyperparameters are configuration values that are set before training a machine learning model and are not learned from data.", "Examples include learning rate, number of layers, and regularization strength."]}}
{"id": "qa_050", "task_type": "qa", "context": "Model evaluation is the process of assessing how well a trained machine learning model performs on unseen data using metrics such as accuracy, precision, recall, F1 score, or mean squared error. Proper evaluation helps determine if a model generalizes well.", "question": "What is model evaluation in machine learning?", "target": {"answer": "Model evaluation is the process of assessing a trained model's performance on unseen data using metrics like accuracy, precision, recall, F1 score, or mean squared error to see if it generalizes well.", "sources": ["Model evaluation is the process of assessing how well a trained machine learning model performs on unseen data using metrics such as accuracy, precision, recall, F1 score, or mean squared error.", "Proper evaluation helps determine if a model generalizes well."]}}
{"id": "mcq_001", "task_type": "mcq", "context": "In supervised learning, a machine learning model is trained using labeled data. Each training example consists of an input and a known output, or label. The goal of the model is to learn a mapping from inputs to outputs so that it can predict labels for new, unseen data.", "question": "What is the main goal of supervised learning?", "target": {"answer": "2", "options": ["To discover hidden groups in unlabeled data.", "To learn a mapping from inputs to outputs so it can predict labels for new data.", "To reduce the dimensionality of the input space.", "To generate new data points without labels."], "sources": ["The goal of the model is to learn a mapping from inputs to outputs so that it can predict labels for new, unseen data."]}}
{"id": "mcq_002", "task_type": "mcq", "context": "Overfitting occurs when a model learns the training data too well, including noise and outliers. Such a model performs very well on the training set but poorly on new, unseen data because it fails to generalize.", "question": "What is a key characteristic of an overfitted model?", "target": {"answer": "3", "options": ["It performs poorly on both training and test data.", "It performs well on test data but poorly on training data.", "It performs very well on training data but poorly on unseen data.", "It has very few parameters and cannot capture patterns."], "sources": ["Overfitting occurs when a model learns the training data too well, including noise and outliers.", "Such a model performs very well on the training set but poorly on new, unseen data because it fails to generalize."]}}
{"id": "mcq_003", "task_type": "mcq", "context": "A stack is a linear data structure that follows the Last In, First Out principle. The most recently added element is the first to be removed. Operations include push to add an element and pop to remove the top element.", "question": "Which principle does a stack follow?", "target": {"answer": "1", "options": ["Last In, First Out", "First In, First Out", "Random Access", "Shortest Job First"], "sources": ["A stack is a linear data structure that follows the Last In, First Out principle."]}}
{"id": "mcq_004", "task_type": "mcq", "context": "A queue is a linear data structure that follows the First In, First Out principle. Elements are added at the rear using enqueue and removed from the front using dequeue.", "question": "Which principle does a queue follow?", "target": {"answer": "4", "options": ["Last In, First Out", "Random Access", "Priority Based", "First In, First Out"], "sources": ["A queue is a linear data structure that follows the First In, First Out principle."]}}
{"id": "mcq_005", "task_type": "mcq", "context": "Gradient descent is an optimization algorithm used to minimize a loss function by iteratively updating parameters in the direction opposite to the gradient. The learning rate controls how large each update step is.", "question": "What does gradient descent try to do during training?", "target": {"answer": "2", "options": ["Maximize the loss function.", "Minimize the loss function.", "Randomly jump between parameter values.", "Increase the learning rate each step."], "sources": ["Gradient descent is an optimization algorithm used to minimize a loss function by iteratively updating parameters in the direction opposite to the gradient."]}}
{"id": "mcq_006", "task_type": "mcq", "context": "Precision in classification is the ratio of true positives to the sum of true positives and false positives. It measures how many of the predicted positive instances are actually positive.", "question": "What does precision measure in a classification model?", "target": {"answer": "3", "options": ["The proportion of actual positives that were correctly identified.", "The overall accuracy of the model for all classes.", "The proportion of predicted positives that are actually positive.", "The error rate on the negative class only."], "sources": ["Precision in classification is the ratio of true positives to the sum of true positives and false positives.", "It measures how many of the predicted positive instances are actually positive."]}}
{"id": "mcq_007", "task_type": "mcq", "context": "Recall in classification is the ratio of true positives to the sum of true positives and false negatives. It measures how many of the actual positive instances the model correctly identifies.", "question": "What does recall focus on in classification?", "target": {"answer": "1", "options": ["How many actual positives are correctly identified.", "How many predicted positives are correct.", "The average of precision and accuracy.", "The number of negative samples misclassified."], "sources": ["Recall in classification is the ratio of true positives to the sum of true positives and false negatives.", "It measures how many of the actual positive instances the model correctly identifies."]}}
{"id": "mcq_008", "task_type": "mcq", "context": "F1 score is the harmonic mean of precision and recall. It is particularly useful when the dataset has class imbalance because it balances both precision and recall in a single metric.", "question": "Why is the F1 score useful for imbalanced datasets?", "target": {"answer": "4", "options": ["It only measures precision.", "It only measures recall.", "It ignores minority classes.", "It balances precision and recall in a single metric."], "sources": ["F1 score is the harmonic mean of precision and recall.", "It is particularly useful when the dataset has class imbalance because it balances both precision and recall in a single metric."]}}
{"id": "mcq_009", "task_type": "mcq", "context": "Normalization in databases is the process of organizing data to reduce redundancy and improve data integrity. It usually involves dividing a database into multiple related tables and defining relationships between them.", "question": "What is the main purpose of normalization in databases?", "target": {"answer": "1", "options": ["To reduce redundancy and improve data integrity.", "To increase query response time.", "To store all data in a single table.", "To remove relationships between tables."], "sources": ["Normalization in databases is the process of organizing data to reduce redundancy and improve data integrity."]}}
{"id": "mcq_010", "task_type": "mcq", "context": "A primary key in a relational database is a field or combination of fields that uniquely identifies each record in a table. A primary key cannot contain null values.", "question": "What is a primary key required to do in a table?", "target": {"answer": "2", "options": ["Contain duplicate values.", "Uniquely identify each record and be non-null.", "Hold foreign key references.", "Store only numeric data."], "sources": ["A primary key in a relational database is a field or combination of fields that uniquely identifies each record in a table.", "A primary key cannot contain null values."]}}
{"id": "mcq_011", "task_type": "mcq", "context": "A foreign key in a relational database is a field in one table that refers to the primary key of another table. It is used to enforce referential integrity between the two tables.", "question": "What is the role of a foreign key in a database?", "target": {"answer": "3", "options": ["To uniquely identify records in its own table.", "To speed up query execution.", "To refer to a primary key in another table and enforce referential integrity.", "To store unstructured data in binary form."], "sources": ["A foreign key in a relational database is a field in one table that refers to the primary key of another table.", "It is used to enforce referential integrity between the two tables."]}}
{"id": "mcq_012", "task_type": "mcq", "context": "Latency in computer networks refers to the time it takes for data to travel from the source to the destination. High latency can cause noticeable delays in real time applications.", "question": "What does high latency in a network cause?", "target": {"answer": "4", "options": ["Higher bandwidth.", "Lower packet loss.", "Increased storage space.", "Noticeable delays in communication."], "sources": ["Latency in computer networks refers to the time it takes for data to travel from the source to the destination.", "High latency can cause noticeable delays in real time applications."]}}
{"id": "mcq_013", "task_type": "mcq", "context": "Bandwidth is the maximum rate at which data can be transmitted over a communication channel, usually measured in bits per second. Higher bandwidth allows more data to be sent in a given time period.", "question": "What does higher bandwidth in a network allow?", "target": {"answer": "1", "options": ["More data to be sent in a given time period.", "Lower latency regardless of distance.", "Automatic error correction of all packets.", "Unlimited number of connected devices."], "sources": ["Bandwidth is the maximum rate at which data can be transmitted over a communication channel, usually measured in bits per second.", "Higher bandwidth allows more data to be sent in a given time period."]}}
{"id": "mcq_014", "task_type": "mcq", "context": "HTTP, or Hypertext Transfer Protocol, is an application-level protocol used for transferring hypertext documents such as HTML. It follows a client server model and is the foundation of data communication on the World Wide Web.", "question": "What is HTTP primarily used for?", "target": {"answer": "3", "options": ["Encrypting network packets at the transport layer.", "Allocating IP addresses to devices.", "Transferring hypertext documents on the World Wide Web.", "Managing database transactions on a server."], "sources": ["HTTP, or Hypertext Transfer Protocol, is an application-level protocol used for transferring hypertext documents such as HTML.", "It follows a client server model and is the foundation of data communication on the World Wide Web."]}}
{"id": "mcq_015", "task_type": "mcq", "context": "An operating system is system software that manages computer hardware and software resources and provides common services for programs. It handles process management, memory management, file systems, and input output control.", "question": "Which of the following is a primary role of an operating system?", "target": {"answer": "2", "options": ["Compiling high-level code to machine code.", "Managing hardware and software resources and providing services to programs.", "Designing user interfaces for web applications.", "Encrypting emails between users."], "sources": ["An operating system is system software that manages computer hardware and software resources and provides common services for programs.", "It handles process management, memory management, file systems, and input output control."]}}
{"id": "mcq_016", "task_type": "mcq", "context": "A hash table is a data structure that maps keys to values using a hash function. The hash function computes an index into an array of buckets, where the value is stored, enabling average case constant time operations.", "question": "What is the advantage of using a hash table?", "target": {"answer": "1", "options": ["Average case constant time operations for search, insert, and delete.", "Guaranteed sorted order of elements.", "Real time encryption of data.", "Automatic backup of stored values."], "sources": ["A hash table is a data structure that maps keys to values using a hash function.", "The hash function computes an index into an array of buckets, where the value is stored, enabling average case constant time operations."]}}
{"id": "mcq_017", "task_type": "mcq", "context": "Encapsulation in object-oriented programming is the practice of bundling data and methods that operate on the data within one unit, such as a class, and restricting direct access to some of the object's components.", "question": "What is the main benefit of encapsulation?", "target": {"answer": "4", "options": ["It guarantees code runs faster.", "It automatically distributes computations.", "It eliminates the need for classes.", "It protects data by restricting direct access and bundling it with related methods."], "sources": ["Encapsulation in object-oriented programming is the practice of bundling data and methods that operate on the data within one unit, such as a class, and restricting direct access to some of the object's components."]}}
{"id": "mcq_018", "task_type": "mcq", "context": "Inheritance allows a class, called a derived class, to acquire properties and behaviors from another class, called a base class. This promotes code reuse and hierarchical relationships.", "question": "What does inheritance enable in object-oriented programming?", "target": {"answer": "2", "options": ["Executing code in parallel.", "A derived class to reuse properties and behaviors of a base class.", "Encrypting object data automatically.", "Storing objects in a database."], "sources": ["Inheritance allows a class, called a derived class, to acquire properties and behaviors from another class, called a base class.", "This promotes code reuse and hierarchical relationships."]}}
{"id": "mcq_019", "task_type": "mcq", "context": "Polymorphism in object-oriented programming allows different classes to be treated through the same interface, where the same operation can exhibit different behavior depending on the object it is applied to.", "question": "What does polymorphism allow in object-oriented programming?", "target": {"answer": "3", "options": ["Storing multiple objects in an array.", "Compiling code to multiple platforms.", "The same operation to behave differently on different classes through a common interface.", "Encrypting communication between objects."], "sources": ["Polymorphism in object-oriented programming allows different classes to be treated through the same interface, where the same operation can exhibit different behavior depending on the object it is applied to."]}}
{"id": "mcq_020", "task_type": "mcq", "context": "Dropout is a regularization technique for neural networks where randomly selected neurons are ignored during training. This reduces co-adaptation between neurons and helps the model generalize better.", "question": "Why is dropout used in neural networks?", "target": {"answer": "4", "options": ["To increase the number of parameters.", "To speed up training by skipping backpropagation.", "To always use all neurons in each layer.", "To reduce co-adaptation of neurons and improve generalization."], "sources": ["Dropout is a regularization technique for neural networks where randomly selected neurons are ignored during training.", "This reduces co-adaptation between neurons and helps the model generalize better."]}}
{"id": "mcq_021", "task_type": "mcq", "context": "Regularization techniques add a penalty term to the loss function to discourage overly complex models. By penalizing large parameter values, regularization helps prevent overfitting.", "question": "How does regularization help a model?", "target": {"answer": "1", "options": ["By discouraging overly complex models and reducing overfitting.", "By eliminating the need for training data.", "By increasing the size of the model parameters.", "By always improving training accuracy to 100 percent."], "sources": ["Regularization techniques add a penalty term to the loss function to discourage overly complex models.", "By penalizing large parameter values, regularization helps prevent overfitting."]}}
{"id": "mcq_022", "task_type": "mcq", "context": "Cross validation evaluates a model by splitting the data into folds. The model is trained on some folds and validated on the remaining fold, and this is repeated so that each fold is used for validation once. The results are averaged to estimate performance.", "question": "What is the purpose of cross validation?", "target": {"answer": "2", "options": ["To reduce the size of the dataset.", "To obtain a more reliable estimate of model performance by averaging results across folds.", "To always improve training accuracy.", "To automatically tune hyperparameters without evaluation."], "sources": ["Cross validation evaluates a model by splitting the data into folds.", "The results are averaged to estimate performance."]}}
{"id": "mcq_023", "task_type": "mcq", "context": "The bias variance trade off describes the balance between underfitting and overfitting. High bias models are too simple and underfit; high variance models are too complex and overfit.", "question": "What does the bias variance trade off describe?", "target": {"answer": "3", "options": ["The trade off between speed and memory.", "The trade off between precision and recall.", "The balance between underfitting due to high bias and overfitting due to high variance.", "The trade off between labeled and unlabeled data."], "sources": ["The bias variance trade off describes the balance between underfitting and overfitting.", "High bias models are too simple and underfit; high variance models are too complex and overfit."]}}
{"id": "mcq_024", "task_type": "mcq", "context": "Tokenization splits text into smaller units called tokens, such as words, subwords, or characters. These tokens are the basic units that NLP models operate on.", "question": "What is the goal of tokenization in NLP?", "target": {"answer": "1", "options": ["To split text into smaller units called tokens that models can process.", "To translate text into another language.", "To remove all punctuation from text.", "To compress text for storage."], "sources": ["Tokenization splits text into smaller units called tokens, such as words, subwords, or characters.", "These tokens are the basic units that NLP models operate on."]}}
{"id": "mcq_025", "task_type": "mcq", "context": "Stemming reduces words to a root form by removing suffixes, while lemmatization uses vocabulary and morphology to return valid base forms called lemmas.", "question": "How does lemmatization differ from stemming?", "target": {"answer": "4", "options": ["Lemmatization always produces shorter words.", "Lemmatization removes all vowels from words.", "Lemmatization is faster but less accurate than stemming.", "Lemmatization uses vocabulary and morphology to produce valid base forms, whereas stemming just strips suffixes."], "sources": ["Stemming reduces words to a root form by removing suffixes, while lemmatization uses vocabulary and morphology to return valid base forms called lemmas."]}}
{"id": "mcq_026", "task_type": "mcq", "context": "A confusion matrix shows counts of true positives, false positives, true negatives, and false negatives for a classification model. It summarizes how often each class is correctly or incorrectly predicted.", "question": "What does a confusion matrix display?", "target": {"answer": "2", "options": ["The loss values over epochs.", "Counts of true positives, false positives, true negatives, and false negatives.", "The ROC curve for each class.", "The learning rate schedule over time."], "sources": ["A confusion matrix shows counts of true positives, false positives, true negatives, and false negatives for a classification model.", "It summarizes how often each class is correctly or incorrectly predicted."]}}
{"id": "mcq_027", "task_type": "mcq", "context": "Principal component analysis transforms original correlated features into a smaller set of uncorrelated components that capture the maximum variance in the data.", "question": "Why is PCA commonly used?", "target": {"answer": "3", "options": ["To increase the number of original features.", "To evaluate classification accuracy.", "To reduce dimensionality by creating uncorrelated components that capture most variance.", "To perform clustering directly without preprocessing."], "sources": ["Principal component analysis transforms original correlated features into a smaller set of uncorrelated components that capture the maximum variance in the data."]}}
{"id": "mcq_028", "task_type": "mcq", "context": "In k-means clustering, the algorithm partitions data into k clusters by minimizing the within cluster sum of squared distances between points and their cluster centroids.", "question": "What objective does k-means clustering try to minimize?", "target": {"answer": "1", "options": ["The within cluster sum of squared distances.", "The number of clusters.", "The number of iterations used.", "The dimensionality of the data directly."], "sources": ["In k-means clustering, the algorithm partitions data into k clusters by minimizing the within cluster sum of squared distances between points and their cluster centroids."]}}
{"id": "mcq_029", "task_type": "mcq", "context": "Bagging trains multiple models on different bootstrap samples of the training data and aggregates their predictions to reduce variance and prevent overfitting.", "question": "What is the main benefit of bagging?", "target": {"answer": "2", "options": ["It always reduces bias to zero.", "It reduces variance and helps prevent overfitting by aggregating models trained on bootstrap samples.", "It guarantees perfect accuracy on training data.", "It removes the need for validation data."], "sources": ["Bagging trains multiple models on different bootstrap samples of the training data and aggregates their predictions to reduce variance and prevent overfitting."]}}
{"id": "mcq_030", "task_type": "mcq", "context": "Boosting trains weak learners sequentially, where each new learner focuses on the mistakes of the previous ones. Their predictions are combined to form a strong learner.", "question": "How does boosting build a strong learner?", "target": {"answer": "4", "options": ["By training one large model from the start.", "By training models independently in parallel.", "By removing data points that are misclassified.", "By training weak learners sequentially, each focusing on previous errors, and combining their predictions."], "sources": ["Boosting trains weak learners sequentially, where each new learner focuses on the mistakes of the previous ones.", "Their predictions are combined to form a strong learner."]}}
{"id": "mcq_031", "task_type": "mcq", "context": "In k-nearest neighbors, the hyperparameter k specifies how many neighbors are considered when making a prediction. Small k can lead to noisy decisions, while large k can oversmooth class boundaries.", "question": "What effect does choosing a very large k in k-nearest neighbors have?", "target": {"answer": "1", "options": ["It oversmooths class boundaries, possibly missing local structure.", "It makes the model extremely sensitive to noise.", "It reduces the size of the training set.", "It guarantees zero training error."], "sources": ["In k-nearest neighbors, the hyperparameter k specifies how many neighbors are considered when making a prediction.", "Small k can lead to noisy decisions, while large k can oversmooth class boundaries."]}}
{"id": "mcq_032", "task_type": "mcq", "context": "A decision tree splits data based on feature values, creating internal nodes as decision tests and leaves as predictions. It learns decision rules from features to make predictions.", "question": "What does a decision tree learn to make predictions?", "target": {"answer": "3", "options": ["Random clusters of data points.", "New features independent of the original ones.", "Decision rules that split data based on feature values.", "Neural network weights and biases."], "sources": ["A decision tree splits data based on feature values, creating internal nodes as decision tests and leaves as predictions.", "It learns decision rules from features to make predictions."]}}
{"id": "mcq_033", "task_type": "mcq", "context": "Batch normalization normalizes the inputs of each layer for each mini batch, reducing internal covariate shift and helping stabilize and speed up training.", "question": "What is one major benefit of batch normalization?", "target": {"answer": "2", "options": ["It replaces the need for activation functions.", "It stabilizes and speeds up training by normalizing layer inputs for each mini batch.", "It guarantees zero validation error.", "It eliminates the need for a learning rate."], "sources": ["Batch normalization normalizes the inputs of each layer for each mini batch, reducing internal covariate shift and helping stabilize and speed up training."]}}
{"id": "mcq_034", "task_type": "mcq", "context": "A learning rate scheduler adjusts the learning rate during training, for example by reducing it when validation loss stops improving, which can help the model converge more effectively.", "question": "Why might a learning rate scheduler reduce the learning rate during training?", "target": {"answer": "4", "options": ["To permanently stop training.", "To increase the model size.", "To make gradients larger.", "To help the model converge better when validation loss stops improving."], "sources": ["A learning rate scheduler adjusts the learning rate during training, for example by reducing it when validation loss stops improving, which can help the model converge more effectively."]}}
{"id": "mcq_035", "task_type": "mcq", "context": "Hyperparameters are configuration values such as learning rate, number of layers, and regularization strength that are set before training and not learned from data.", "question": "Which of the following is a hyperparameter?", "target": {"answer": "1", "options": ["Learning rate.", "Model prediction on a test sample.", "Computed loss on a batch.", "Gradient values during backpropagation."], "sources": ["Hyperparameters are configuration values such as learning rate, number of layers, and regularization strength that are set before training and not learned from data."]}}
{"id": "mcq_036", "task_type": "mcq", "context": "Model evaluation assesses a trained model on unseen data using metrics such as accuracy, precision, recall, F1 score, or mean squared error to estimate how well it generalizes.", "question": "What is the main purpose of model evaluation?", "target": {"answer": "3", "options": ["To modify the training data.", "To change the optimizer automatically.", "To assess how well the trained model generalizes to unseen data using metrics.", "To increase the number of model parameters."], "sources": ["Model evaluation assesses a trained model on unseen data using metrics such as accuracy, precision, recall, F1 score, or mean squared error to estimate how well it generalizes."]}}
{"id": "mcq_037", "task_type": "mcq", "context": "Unsupervised learning uses unlabeled data to discover hidden patterns or structures such as clusters or associations without explicit output labels.", "question": "What type of data does unsupervised learning use?", "target": {"answer": "2", "options": ["Only labeled data.", "Unlabeled data without explicit outputs.", "Streaming data from sensors only.", "Encrypted labeled data."], "sources": ["Unsupervised learning uses unlabeled data to discover hidden patterns or structures such as clusters or associations without explicit output labels."]}}
{"id": "mcq_038", "task_type": "mcq", "context": "A learning curve plots model performance on training and validation sets as training progresses, helping diagnose overfitting and underfitting.", "question": "What can a learning curve help you detect?", "target": {"answer": "4", "options": ["The exact optimal hyperparameters.", "The need for more features.", "The exact training time in seconds.", "Signs of overfitting or underfitting by comparing training and validation performance."], "sources": ["A learning curve plots model performance on training and validation sets as training progresses, helping diagnose overfitting and underfitting."]}}
{"id": "mcq_039", "task_type": "mcq", "context": "The silhouette score ranges from minus one to one and measures how similar an object is to its own cluster compared to other clusters. Higher values indicate better clustering for that point.", "question": "What does a high silhouette score indicate for a data point?", "target": {"answer": "1", "options": ["It is well matched to its own cluster and poorly matched to other clusters.", "It should be removed from the dataset.", "It must be an outlier.", "It belongs to multiple clusters equally."], "sources": ["The silhouette score ranges from minus one to one and measures how similar an object is to its own cluster compared to other clusters.", "Higher values indicate better clustering for that point."]}}
{"id": "mcq_040", "task_type": "mcq", "context": "Over-parameterization describes models that have more parameters than needed to fit the training data, which can still generalize with proper regularization and optimization.", "question": "What describes an over-parameterized model?", "target": {"answer": "2", "options": ["A model with no trainable parameters.", "A model with more parameters than needed to fit the training data.", "A model that has fewer parameters than the number of training samples.", "A model that cannot overfit the data."], "sources": ["Over-parameterization describes models that have more parameters than needed to fit the training data, which can still generalize with proper regularization and optimization."]}}
{"id": "mcq_041", "task_type": "mcq", "context": "An interpreter executes high-level source code directly by reading it line by line, translating, and running it without creating separate machine code files.", "question": "How does an interpreter run a program?", "target": {"answer": "3", "options": ["By first compiling to machine code and then executing it.", "By converting code to bytecode only.", "By directly reading and executing high-level code line by line.", "By sending code to a remote server for execution."], "sources": ["An interpreter executes high-level source code directly by reading it line by line, translating, and running it without creating separate machine code files."]}}
{"id": "mcq_042", "task_type": "mcq", "context": "A compiler translates source code written in a high-level language into machine code or an intermediate form that can be executed by a computer, typically through steps like lexical analysis, parsing, and code generation.", "question": "What is the main task of a compiler?", "target": {"answer": "1", "options": ["Translating high-level source code into machine code or an intermediate executable form.", "Executing high-level code line by line without translation.", "Storing source code in a database.", "Optimizing hardware components."], "sources": ["A compiler translates source code written in a high-level language into machine code or an intermediate form that can be executed by a computer."]}}
{"id": "mcq_043", "task_type": "mcq", "context": "Time complexity expresses how an algorithm's running time grows with input size, often described using Big O notation to focus on dominant terms and ignore constants.", "question": "What does time complexity describe?", "target": {"answer": "4", "options": ["The exact runtime in seconds.", "The memory usage of an algorithm.", "The compilation time of a program.", "How running time grows as the input size increases."], "sources": ["Time complexity expresses how an algorithm's running time grows with input size, often described using Big O notation to focus on dominant terms and ignore constants."]}}
{"id": "mcq_044", "task_type": "mcq", "context": "Space complexity measures the amount of memory an algorithm uses as a function of input size, including memory for input, output, and auxiliary computations.", "question": "What does space complexity focus on?", "target": {"answer": "2", "options": ["The number of CPU cores used.", "The amount of memory an algorithm uses as input size grows.", "The disk space needed to store source code.", "The size of the compiled executable file."], "sources": ["Space complexity measures the amount of memory an algorithm uses as a function of input size, including memory for input, output, and auxiliary computations."]}}
{"id": "mcq_045", "task_type": "mcq", "context": "In natural language processing, tokenization, stemming, lemmatization, and normalization are common preprocessing steps to convert raw text into a form suitable for modeling.", "question": "Which of the following is a common text preprocessing step in NLP?", "target": {"answer": "1", "options": ["Tokenization.", "Hyperparameter tuning.", "Weight initialization.", "Network pruning."], "sources": ["In natural language processing, tokenization, stemming, lemmatization, and normalization are common preprocessing steps to convert raw text into a form suitable for modeling."]}}
{"id": "mcq_046", "task_type": "mcq", "context": "Batch size is the number of training examples used in one forward and backward pass of the model. Smaller batch sizes can provide more frequent updates, while larger ones can make better use of hardware.", "question": "What is batch size in training a model?", "target": {"answer": "3", "options": ["The number of layers in the model.", "The number of epochs to train for.", "The number of training examples processed before updating model parameters.", "The number of neurons in the output layer."], "sources": ["Batch size is the number of training examples used in one forward and backward pass of the model."]}}
{"id": "mcq_047", "task_type": "mcq", "context": "Epoch refers to one complete pass through the entire training dataset during training. Multiple epochs are often needed for a model to converge.", "question": "What does one epoch represent in model training?", "target": {"answer": "4", "options": ["One gradient update.", "One batch of data processed.", "One forward pass only.", "One complete pass through the entire training dataset."], "sources": ["Epoch refers to one complete pass through the entire training dataset during training."]}}
{"id": "mcq_048", "task_type": "mcq", "context": "Mean squared error is a common loss function for regression that computes the average of the squared differences between predicted and true values.", "question": "For which type of problem is mean squared error commonly used?", "target": {"answer": "2", "options": ["Classification problems with many classes.", "Regression problems measuring squared differences between predictions and true values.", "Clustering problems with unknown labels.", "Sequence labeling tasks only."], "sources": ["Mean squared error is a common loss function for regression that computes the average of the squared differences between predicted and true values."]}}
{"id": "mcq_049", "task_type": "mcq", "context": "Softmax is an activation function used in the output layer of multi class classification models to convert raw scores into probabilities that sum to one.", "question": "Where is the softmax function typically used?", "target": {"answer": "1", "options": ["In the output layer of multiclass classification models to produce probabilities.", "In the hidden layers of regression models.", "Only in clustering algorithms.", "Only in binary linear regression."], "sources": ["Softmax is an activation function used in the output layer of multi class classification models to convert raw scores into probabilities that sum to one."]}}
{"id": "mcq_050", "task_type": "mcq", "context": "ReLU, or Rectified Linear Unit, is a popular activation function defined as the positive part of its input. It introduces nonlinearity and helps mitigate the vanishing gradient problem.", "question": "What is a key advantage of using ReLU as an activation function?", "target": {"answer": "3", "options": ["It guarantees zero training loss.", "It outputs only negative values.", "It introduces nonlinearity while helping mitigate the vanishing gradient problem.", "It automatically normalizes the input data."], "sources": ["ReLU, or Rectified Linear Unit, is a popular activation function defined as the positive part of its input.", "It introduces nonlinearity and helps mitigate the vanishing gradient problem."]}}
